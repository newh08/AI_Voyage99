
## Q1) 어떤 task를 선택하셨나요?
 MNLI, 두 문장 사이의 관계를 파악하는 모델 설계


## Q2) 모델은 어떻게 설계하셨나요? 설계한 모델의 입력과 출력 형태가 어떻게 되나요?
모델의 입력과 출력 형태 또는 shape을 정확하게 기술
코랩을 사용하지 않고, 로컬에서 돌려보기 위해 DistilBERT 보다 작은 TinyBERT 모델을 선택했습니다.
TinyBERT는 312 개의 노드가 총 4개의 Layer 구조이며 각 레이어는 12개의 attention head (Multi Head Attention) 을 가진 모델 이기 때문에
입력으로 최대 312개의 토큰을 받을 수 있으며 형태는 (batch_size, 312) 이고,
3가지 라벨을 구분해야하기 때문에 출력의 형태는 3개의 값 (batch_size, 3) 입니다.

## Q3) 어떤 pre-trained 모델을 활용하셨나요?
PyTorch에서 위에서 정한 task에 맞는 pre-trained 모델을 선정
BERT 모델이 사전학습시 두개 문장 사이의 관계 학습을 진행한다고 배워, 해당 모델을 사용하려 했지만
하드웨어적인 한계가 있어 훨씬 작은 모델인 TinyBERT 모델을 선정했습니다.

## Q4) 실제로 pre-trained 모델을 fine-tuning했을 때 loss curve은 어떻게 그려지나요? 그리고 pre-train 하지 않은 Transformer를 학습했을 때와 어떤 차이가 있나요? 

과제를 진행하면서 두 가지 문제점이 있었는데
  1. Pre-Train 모델의 정확도가 너무 낮게 (50%이하) 로 나왔습니다 -> TinyBERT 도 사전 학습으로 두 문장 사이의 관계를 학습하는거로 알고있었는데, MNLI 에서 낮은성능을 보여 뭔가 문제가 있다고 생각했습니다.
  2. UnTrain 모델은 정확도가 거의 변하지 않는 모습을 보였습니다.
 
해당 내용을 멘토링을 진행 후
  1. 사전 학습 모델의 파라미터 고정 해제
  2. 학습률(lr) 을 더 작은 값으로 조정(10^-3 에서 10^-5 로 변경)

을 진행해 사전 학습 모델은 정확도를 0.72 까지 올렸고, 학습을 진행하지 않은 모델도 마지막 에폭시에서 정확도가 개선되는 모습을 확인 할 수 있었습니다.
<img width="826" alt="image" src="https://github.com/user-attachments/assets/e9ad2135-6824-4401-a5ae-04205deb48e6" />

