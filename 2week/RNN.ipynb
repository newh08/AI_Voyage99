{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIgpn15a2EOL"
   },
   "source": [
    "# RNN 실습\n",
    "\n",
    "이번 실습에서는 RNN을 사용하여 주어진 영화 리뷰의 감정 분석 모델을 구현하고 학습할 것 입니다. 우선 필요한 library들 부터 import합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5086,
     "status": "ok",
     "timestamp": 1724034322123,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "oXsPIYjtndsf",
    "outputId": "7e4dae26-9730-4cfa-b826-0a860149b7f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (2.21.0)\n",
      "Requirement already satisfied: filelock in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (3.15.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: aiohttp in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (3.10.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.2 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (0.23.5)\n",
      "Requirement already satisfied: packaging in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from aiohttp->datasets) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from aiohttp->datasets) (1.9.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "nQ0boI90Mj5H"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hvPw92WD2Vy9"
   },
   "source": [
    "## IMDB dataset\n",
    "\n",
    "이번에는 영화 리뷰 감정 분석을 위한 dataset인 IMDB를 불러옵니다.\n",
    "Huggingface에서 제공하는 `load_dataset` 함수를 사용하여 다음과 같이 불러올 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6515,
     "status": "ok",
     "timestamp": 1724034334569,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "_b6MXMyX_5hb",
    "outputId": "23cb933e-0fc7-4465-e802-c6d3e44a7392"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f029b7308c1f41bc911add3d38a06771",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75808600c7b64df18fd7cf81f1d7133c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420e4d5205ed4603a5b4a8a6a6d43a46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a787c5908cfd48f98bb72efa7e1b1e90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d167dfb2eeb74a99820141bf393bde60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01bcfd62563a4d02befee736ecde4644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493192bbeba640e099eec72dcef9b915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9KZfSfA22jg"
   },
   "source": [
    "이외에도 다양한 dataset들을 다운받을 수 있습니다. 다음 링크에서 dataset 목록을 확인할 수 있습니다: https://huggingface.co/datasets.\n",
    "\n",
    "이번에는 data 형태를 확인하기 위해 train data의 첫 번째 data를 한 번 확인해봅시다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1724034334569,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "yRU-oqJ_AO6C",
    "outputId": "2d17027b-b44a-4e6f-e308-0ccb00603b29"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7cWsV8W_3Nt9"
   },
   "source": [
    "보시다시피 각각의 data는 text와 긍정적인지 부정적인지 의미하는 label로 구성되어있습니다.\n",
    "여기서 label은 0 또는 1의 값을 가집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJeBOWwE2Z0p"
   },
   "source": [
    "## Tokenizer 제작\n",
    "\n",
    "이번에는 tokenizer를 huggingface library를 활용하여 직접 만들어 볼 것입니다.\n",
    "다음과 같이 필요한 library들을 import하고 tokenizer model을 만듭니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OxIEFi-vxrdR"
   },
   "outputs": [],
   "source": [
    "from tokenizers import (\n",
    "    decoders,\n",
    "    models,\n",
    "    normalizers,\n",
    "    pre_tokenizers,\n",
    "    processors,\n",
    "    trainers,\n",
    "    Tokenizer,\n",
    ")\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(models.WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tVKfSr8y3fvb"
   },
   "source": [
    "이번에는 WordPiece를 사용할 것 입니다.\n",
    "WordPiece 이외의 모델들도 많이 있습니다. 이전 챕터에서 배운 BPE 또한 지원을 하고 있습니다.\n",
    "지원하고 있는 tokenizer model들은 다음 링크에서 확인하실 수 있습니다:\n",
    "https://huggingface.co/docs/tokenizers/api/models.\n",
    "\n",
    "일반적으로 tokenizer는 text를 token들로 변환할 때 전부 소문자로 바꾼다든지 특수 문자를 지운다든지 등의 normalization 과정을 거칩니다.\n",
    "그리고 normalization을 거친 text를 어떻게 쪼갤 것인지 정의해줘야 합니다.\n",
    "각각에 대한 요소를 구현한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "TkNINlgpxy5F"
   },
   "outputs": [],
   "source": [
    "tokenizer.normalizer = normalizers.BertNormalizer(lowercase=True)\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozEcAnux4kea"
   },
   "source": [
    "`BertNormalizer`는 소문자 변환이나 기타 무의미한 특수 문자들을 제거해줍니다.\n",
    "그리고 `BertPreTokenizer`는 BERT에서 사용하는 방식대로 text를 쪼갭니다. 자세한 설명은 뒤의 챕터에서 BERT를 설명할 때 해드리겠습니다.\n",
    "\n",
    "마지막으로 우리가 가지고 있는 데이터 `ds['train']`을 가지고 위에서 정의한 tokenizer의 사전을 학습합니다. 이를 구현한 것은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 20682,
     "status": "ok",
     "timestamp": 1724034355243,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "A2lBAjc04yMv",
    "outputId": "75d1a808-287e-4216-838d-20dfb44bc6a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_training_corpus():\n",
    "  for i in range(0, len(ds['train']), 1000):\n",
    "    yield ds['train'][i : i + 1000]['text']\n",
    "\n",
    "\n",
    "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\"]\n",
    "trainer = trainers.WordPieceTrainer(vocab_size=10000, special_tokens=special_tokens)\n",
    "tokenizer.train_from_iterator(get_training_corpus(), trainer=trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AncCtNe65PxI"
   },
   "source": [
    "먼저 train data를 1000개씩 쪼개어 넘겨주는 `get_training_corpus` 함수를 선언합니다. 여기서 한 번에 넘기지 않고 쪼개어 넘겨주는 이유는 효율을 위한 것입니다. 그래서 1000 이외의 숫자를 쓰셔도 무방합니다.\n",
    "\n",
    "다음은 WordPiece model을 학습할 수 있는 trainer를 선언하고 `.train_from_iterator` 함수를 통해 tokenizer를 학습하는 모습입니다.\n",
    "학습한 결과는 다음과 같이 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1724034355243,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "MnEFbmg-5rgL",
    "outputId": "2a0fb5b1-65a1-4df1-f11c-0d7f9fd66fbb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=4, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Hello, world!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QwmNL7_g54qW"
   },
   "source": [
    "마지막으로 tokenizer의 성능 향상을 위해 다음과 같이 `BertTokenizerFast`로 wrapping 해줍니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1724034356147,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "nGvo1o2U0CTd",
    "outputId": "7574ba0d-08bd-40d7-8d1d-9d46f2408080"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yunhyeokchoi/Library/Caches/pypoetry/virtualenvs/langchain-kr-_ls1lJm0-py3.11/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizerFast\n",
    "\n",
    "\n",
    "tokenizer = BertTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBwvs9UA5_nW"
   },
   "source": [
    "Wrapping한 tokenizer로 text list가 주어졌을 때 tokenizer하는 코드는 다음과 같이 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1724034356148,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "CBL54JLkzFOE",
    "outputId": "54e33d82-42c5-4388-c50d-29c34748949c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens: {'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "Padding token ID: 1\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "text_list = [\n",
    "  \"Hello, y'all!\",\n",
    "  \"How are you 😁 ?\",\n",
    "  \"I'm fine thank you and you?\"\n",
    "]\n",
    "\n",
    "tokens = tokenizer(text_list, padding=True)\n",
    "tokens.input_ids\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 사용 중인 토크나이저 확인\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "print(\"Padding token ID:\", tokenizer.pad_token_id)\n",
    "print(\"Vocabulary size:\", tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pepv32Zb6YnI"
   },
   "source": [
    "Tokenizer의 사전의 크기는 다음과 같이 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1724034356148,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "5LH5jGgK0a3g",
    "outputId": "5cc7bc56-6c25-4dde-8682-359cba3869f2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10001"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "919T7Rsh6cYt"
   },
   "source": [
    "마지막으로 tokenizer의 특수 token들과 그 index는 다음과 같이 접근할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1724034356148,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "z-OLE0zs6PJH",
    "outputId": "328568e1-0e68-4e9b-e534-568f1aea0ad9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UNK] 0\n",
      "[PAD] 1\n",
      "[CLS] 2\n",
      "[SEP] 3\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.unk_token, tokenizer.unk_token_id)\n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id)\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id)\n",
    "print(tokenizer.sep_token, tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4vBU7XU15p3"
   },
   "source": [
    "다음과 같이 tokenizer를 활용하여 data 별 token 개수의 분포를 확인할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 228
    },
    "executionInfo": {
     "elapsed": 25907,
     "status": "ok",
     "timestamp": 1724034382052,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "t2MtSoBLJB6c",
    "outputId": "146138b3-8d0a-4015-dfad-0d7ffc7d398b"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASIAAADTCAYAAAAlBx6+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWrklEQVR4nO3df0zU9/0H8Ccgd6bkDjHCnTCr3RQYQjVSS3FVKgx/BKxzXSFzSTGLRmp/x0jLlnTauaJuFTekdat2cUpMFq3LAnUgjRlTT2pxUyLKnBFqgTtU4O7Ag0N8ff/ol0/9KCqH4Bvh+UjeSe/zft3n836p9+znPnd88AMgICJSyF/1AoiIGEREpByDiIiUYxARkXIMIiJSjkFERMoxiIhIuTGqFzCUwsPD4Xa7VS+DaNQymUxobGy8b92IDaLw8HA0NDSoXgbRqBcREXHfMBqxQdR7JhQREcGzIiIFTCYTGhoa+vX6G7FB1MvtdjOIiIY5XqwmIuUYRESkHIOIiJRjEBGRcgwiIlJuxH9qNhg+qLbpHq+NS1S0EqKRiWdERKQcg4iIlGMQEZFyDCIiUo5BRETKMYiISDkGEREpxyAiIuUYRESkHIOIiJRjEBGRcgwiIlKOQUREyjGIiEg5BhERKccgIiLlGEREpByDiIiUYxARkXIMIiJSjkFERMoxiIhIOQYRESnHICIi5XwOovDwcOzZswdXr17F9evXcebMGcTHx+tqNmzYgMbGRly/fh2HDx/G1KlTdfMhISHYu3cvnE4nWltbsXPnTgQFBelq4uLiUFFRAY/Hg6+++grr1q0bQHtE9CjwKYjGjRuHY8eOobu7G4sXL0ZMTAzWrl2L1tZWrSYnJwevv/46srOzkZCQgI6ODpSWlsJoNGo1RUVFmD59OlJTU5Geno558+bhT3/6kzZvMplQVlaG+vp6xMfHY926dVi/fj1WrVo1CC0T0XAk/R15eXlSUVFxz5rGxkZZu3at9thsNovH45HMzEwBINHR0SIiEh8fr9UsXLhQenp6ZOLEiQJAsrOz5dq1axIYGKg79rlz5/q9VpPJJCIiJpOp38+52/ig2qYbD7o/Do7RMHx5Dfp0RvT888/jyy+/xF//+lc4HA6cOnUKK1eu1OafeOIJTJw4EeXl5do2l8uFyspKJCZ+8/viExMT0draiqqqKq2mvLwcN2/eREJCglZTUVGB7u5uraa0tBTR0dEYN25cn2szGAwwmUy6QUSPBp+C6Lvf/S5efvllXLhwAQsXLsRHH32EP/zhD3jppZcAAFarFQDgcDh0z3M4HNqc1WpFc3Ozbr6npwctLS26mr72cesxbpebmwuXy6WNhoYGX1ojIoV8CiJ/f3+cOnUKv/zlL/Gf//wHH3/8MT7++GNkZ2cP1fr6LS8vD2azWRsRERGql0RE/eRTEDU1NaGmpka37dy5c3j88ccBAHa7HQBgsVh0NRaLRZuz2+0ICwvTzQcEBGD8+PG6mr72cesxbuf1euF2u3WDiB4NPgXRsWPHEBUVpdsWGRmJ+vp6AMClS5fQ1NSElJQUbd5kMiEhIQE2mw0AYLPZEBISglmzZmk1ycnJ8Pf3R2VlpVYzb948jBkzRqtJTU3F+fPn0dbW5luHRPRI6PdV8Keeekq8Xq/k5ubK9773PfnpT38q7e3tsnz5cq0mJydHWlpaZMmSJRIbGysHDx6UixcvitFo1Go+++wzqaqqktmzZ8ucOXOktrZWioqKtHmz2SxNTU2ye/duiYmJkYyMDGlvb5dVq1YNyRX7+w1+asbB4fvw8TXo287T0tLkzJkz4vF4pKamRlauXHlHzYYNG6SpqUk8Ho8cPnxYpk2bppsPCQmRoqIicblc0tbWJrt27ZKgoCBdTVxcnFRUVIjH45HLly9LTk7OUP4h3HMwiDg4fB++vAb9/v8/RhyTyQSXywWz2fzA14s+qLbpHq+NS3yg/RGNBr68BvmzZkSkHIOIiJRjEBGRcgwiIlKOQUREyjGIiEg5BhERKccgIiLlGEREpByDiIiUYxARkXIMIiJSjkFERMoxiIhIOQYRESnHICIi5RhERKQcg4iIlGMQEZFyDCIiUo5BRETKMYiISDkGEREpxyAiIuUYRESkHIOIiJRjEBGRcgwiIlKOQUREyjGIiEg5BhERKccgIiLlGEREpNwDBdHbb78NEUF+fr62zWg0Yvv27bh69Srcbjf279+PsLAw3fMmTZqE4uJidHR0wOFwYMuWLQgICNDVJCUloaqqCp2dnbhw4QKysrIeZKlENIwNOIieeuoprF69GqdPn9Ztz8/Px5IlS/Diiy8iKSkJ4eHh+PTTT789oL8/SkpKYDAYMGfOHGRlZWHFihV47733tJopU6agpKQER44cwcyZM7Ft2zbs3LkTCxYsGOhyiWiYE19HUFCQ1NbWSkpKihw5ckTy8/MFgJjNZunq6pIXXnhBq42KihIRkYSEBAEgixYtkhs3bkhYWJhWs3r1amlra5PAwEABIJs2bZLq6mrdMfft2yeHDh3q9xpNJpOIiJhMJp/7u318UG2753jQ/XNwjMThy2twQGdEhYWFKCkpweeff67bHh8fD4PBgPLycm1bbW0t6uvrkZiYCABITExEdXU1mpubtZrS0lIEBwdj+vTpWs2t++it6d1HXwwGA0wmk24Q0aNhjK9PyMzMxKxZszB79uw75qxWK7q6uuB0OnXbHQ4HrFarVuNwOO6Y7527V01wcDDGjh2Lzs7OO46dm5uL9evX+9oOEQ0DPp0Rfec738Hvf/97/OxnP0NXV9dQrWlA8vLyYDabtREREaF6SUTUTz4FUXx8PCwWC06dOoXu7m50d3fjueeew+uvv47u7m44HA4YjUYEBwfrnmexWGC32wEAdrsdFovljvneuXvVOJ3OPs+GAMDr9cLtdusGET0afAqizz//HLGxsZg5c6Y2Tp48iaKiIsycORNffvklvF4vUlJStOdERkZi8uTJsNlsAACbzYa4uDiEhoZqNampqXA6naipqdFqbt1Hb03vPohoZPHpGlF7ezvOnj2r29bR0YFr165p23ft2oWtW7eipaUFLpcLBQUFOH78OCorKwEAZWVlqKmpwZ49e5CTkwOr1YqNGzeisLAQXq8XALBjxw68+uqr2Lx5Mz755BMkJycjIyMDaWlpg9EzEQ0zPl+svp+33noLN2/exIEDB2A0GlFaWoo1a9Zo8zdv3kR6ejo++ugj2Gw2dHR0YPfu3Xj33Xe1mrq6OqSlpSE/Px9vvPEGvv76a6xcuRJlZWWDvVwiGgb88M3n+COOyWSCy+WC2Wx+4OtFH1Tf+y3h2ri7f62AaLTy5TXInzUjIuUYRESkHIOIiJRjEBGRcgwiIlKOQUREyjGIiEg5BhERKccgIiLlGEREpByDiIiUYxARkXIMIiJSjkFERMoxiIhIOQYRESnHICIi5Qb9VrEjwf3uyEhEg4tnRESkHIOIiJRjEBGRcgwiIlKOQUREyjGIiEg5BhERKcfvEQ2Cvr53xN/+StR/PCMiIuUYRESkHIOIiJRjEBGRcgwiIlKOQUREyjGIiEg5n4LonXfewRdffAGXywWHw4GDBw8iMjJSV2M0GrF9+3ZcvXoVbrcb+/fvR1hYmK5m0qRJKC4uRkdHBxwOB7Zs2YKAgABdTVJSEqqqqtDZ2YkLFy4gKytrgC0S0XDnUxAlJSWhsLAQzzzzDFJTUxEYGIiysjI89thjWk1+fj6WLFmCF198EUlJSQgPD8enn3767QH9/VFSUgKDwYA5c+YgKysLK1aswHvvvafVTJkyBSUlJThy5AhmzpyJbdu2YefOnViwYMEgtExEw40fABnokydMmIArV65g3rx5+Ne//gWz2YwrV65g+fLlOHDgAAAgKioK58+fxzPPPIPKykosWrQIxcXFCA8PR3NzMwBg9erV2Lx5M0JDQ9Hd3Y1NmzYhLS0NcXFx2rH27duHcePGYfHixX2uxWAwwGg0ao9NJhMaGhpgNpvhdrt96msw7tDIb1bTaGcymeByufr1Gnyga0TBwcEAgJaWFgBAfHw8DAYDysvLtZra2lrU19cjMfGbF2ZiYiKqq6u1EAKA0tJSBAcHY/r06VrNrfvorendR19yc3Phcrm00dDQ8CCtEdFDNOAg8vPzw7Zt23D06FGcPXsWAGC1WtHV1QWn06mrdTgcsFqtWo3D4bhjvnfuXjXBwcEYO3Zsn+vJy8uD2WzWRkRExEBbI6KHbMA/9FpYWIjY2Fg8++yzg7meAfN6vfB6vaqXQUQDMKAzooKCAqSnp2P+/Pm6t0B2ux1Go1F7y9bLYrHAbrdrNRaL5Y753rl71TidTnR2dg5kyUQ0jPkcRAUFBVi2bBmSk5NRV1enm6uqqoLX60VKSoq2LTIyEpMnT4bN9s0FYJvNhri4OISGhmo1qampcDqdqKmp0Wpu3UdvTe8+iGhk8emtWWFhIZYvX46lS5fC7XZrZy29Zyoulwu7du3C1q1b0dLSApfLhYKCAhw/fhyVlZUAgLKyMtTU1GDPnj3IycmB1WrFxo0bUVhYqL212rFjB1599VVs3rwZn3zyCZKTk5GRkYG0tLRBbp+IhgOfzojWrFmDcePG4Z///Cfsdrs2MjMztZq33noLxcXFOHDgACoqKmC32/HjH/9Ym7958ybS09PR09MDm82GvXv34i9/+Qveffddraaurg5paWlITU3F6dOnsXbtWqxcuRJlZWWD0DIRDTcP9D2i4cyX7zDcbih+0yu/V0SjzUP7HhER0WBgEBGRcgwiIlKOQUREyjGIiEg5BhERKccgIiLlGEREpBx/5fRDcvuXJPkFR6Jv8YyIiJRjEBGRcgwiIlKO14gwND/kSkT9xzMiIlKOQUREyjGIiEg5XiNShN8rIvoWz4iISDkGEREpxyAiIuUYRESkHC9WDxO8eE2jGc+IiEg5BhERKce3ZsMU36rRaMIzIiJSjkFERMrxrdkjgm/VaCTjGRERKcczokcUz5BoJOEZEREpxzOiEYJnSPQoYxCNUAwmepQM6yBas2YN1q1bB6vVitOnT+O1117DyZMnVS/rkdSfXxDAsCJVhm0QZWRkYOvWrcjOzkZlZSXefPNNlJaWIioqCleuXFG9vBHpfmHFoKKh4gdAVC+iLydOnMDJkyfx2muvAQD8/Pxw+fJlFBQUYPPmzXfUGwwGGI1G7bHJZEJDQwMiIiLgdrvveaz3T3w+uIsnAMAvnklRvQRSqPc1aDab7/saHJZnRIGBgYiPj0deXp62TURQXl6OxMS+/6+cm5uL9evX37G9oaFhqJZJ9/Gqy6V6CTQMmEymRzOIJkyYgDFjxsDhcOi2OxwOREdH9/mcvLw8bN26Vbdt/PjxaGlpuetxfDlretSNll7Z5/BiMpnQ2Nh437phGUQD4fV64fV6ddv6+xfkdruH9V/mYBotvbLP4aG/axuWX2i8evUqbty4AYvFottusVhgt9sVrYqIhsqwDKLu7m5UVVUhJeXbi51+fn5ISUmBzcbfU080EslwHBkZGeLxeOSll16S6Oho2bFjh7S0tEhYWNigHcNgMMivfvUrMRgMyvsd6jFaemWfj+xQvoC7jldeeUXq6uqks7NTTpw4IU8//bTyNXFwcAz+GLbfIyKi0WNYXiMiotGFQUREyjGIiEg5BhERKTeqg2jNmjW4dOkSPB4PTpw4gdmzZ6te0j3NnTsXf//739HQ0AARwdKlS++o2bBhAxobG3H9+nUcPnwYU6dO1c2HhIRg7969cDqdaG1txc6dOxEUFKSriYuLQ0VFBTweD7766iusW7duSPu63TvvvIMvvvgCLpcLDocDBw8eRGRkpK7GaDRi+/btuHr1KtxuN/bv34+wsDBdzaRJk1BcXIyOjg44HA5s2bIFAQEBupqkpCRUVVWhs7MTFy5cQFZW1pD31ys7OxunT5+G0+mE0+nE8ePHsWjRIm1+JPToC+Uf3akYGRkZ0tnZKStWrJDvf//78sc//lFaWlokNDRU+druNhYtWiS//vWv5Uc/+pGIiCxdulQ3n5OTI62trfL8889LXFyc/O1vf5OLFy+K0WjUaj777DP597//LU8//bT84Ac/kP/+979SVFSkzZtMJmlqapI9e/ZITEyMZGZmSkdHh6xateqh9Xno0CHJysqSmJgYefLJJ6W4uFjq6urkscce02o+/PBDqa+vl/nz58usWbPk+PHjcvToUW3e399fzpw5I2VlZTJjxgxZtGiRNDc3y29+8xutZsqUKdLe3i6/+93vJDo6Wl555RXp7u6WBQsWPJQ+09PTZfHixTJ16lSZNm2abNy4Ubq6uiQmJmbE9OjDUL4AJePEiRNSUFCgPfbz85Ovv/5a3n77beVr68/oK4gaGxtl7dq12mOz2Swej0cyMzMFgERHR4uISHx8vFazcOFC6enpkYkTJwoAyc7OlmvXrklgYKBWk5eXJ+fOnVPW64QJE0REZO7cuVpfXV1d8sILL2g1UVFRIiKSkJAgwDehfePGDd0XYFevXi1tbW1ab5s2bZLq6mrdsfbt2yeHDh1S1uu1a9fk5z//+Yjusa8xKt+a9d5mpLy8XNt2v9uMDHdPPPEEJk6cqOvJ5XKhsrJS6ykxMRGtra2oqqrSasrLy3Hz5k0kJCRoNRUVFeju7tZqSktLER0djXHjxj2cZm4THBwMANqdFOLj42EwGHS91tbWor6+XtdrdXU1mpubtZrS0lIEBwdj+vTpWs2t++itUfFvwN/fH5mZmQgKCoLNZhuRPd7LqAyie91mxGq1KlrVg+ld9716slqtun+0ANDT04OWlhZdTV/7uPUYD5Ofnx+2bduGo0eP4uzZs9o6urq64HQ671inL33crSY4OBhjx44dkn5uFxsbC7fbja6uLuzYsQPLli3DuXPnRlSP/TFibgNCI1NhYSFiY2Px7LPPql7KkKitrcXMmTMRHByMn/zkJ9i9ezeSkpJUL+uhG5VnRCPxNiO9675XT3a7/Y5PXQICAjB+/HhdTV/7uPUYD0tBQQHS09Mxf/583Z027XY7jEaj9pbt1nX60sfdapxOJzo7Owe9n750d3fj4sWLOHXqFH7xi1/g9OnTeOONN0ZUj/0xKoNoJN5m5NKlS2hqatL1ZDKZkJCQoPVks9kQEhKCWbNmaTXJycnw9/dHZWWlVjNv3jyMGfPtyXJqairOnz+Ptra2h9MMvgmhZcuWITk5GXV1dbq5qqoqeL1eXa+RkZGYPHmyrte4uDiEhoZqNampqXA6naipqdFqbt1Hb43KfwP+/v4wGo0juse7UX7FXMV4GLcZGewRFBQkM2bMkBkzZoiIyJtvvikzZsyQSZMmCfDNx/ctLS2yZMkSiY2NlYMHD/b58X1VVZXMnj1b5syZI7W1tbqP781mszQ1Ncnu3bslJiZGMjIypL29/aF+fF9YWCitra0yb948sVgs2hg7dqxW8+GHH0pdXZ0899xzMmvWLDl27JgcO3ZMm+/9aPsf//iHPPnkk7JgwQJxOBx9frS9efNmiYqKkpdffvmhfrT9/vvvy9y5c2Xy5MkSGxsr77//vvT09MgPf/jDEdOjD0P5ApSNR+02I0lJSdKXP//5z1rNhg0bpKmpSTwejxw+fFimTZum20dISIgUFRWJy+WStrY22bVrlwQFBelq4uLipKKiQjwej1y+fFlycnIeap93k5WVpdUYjUbZvn27XLt2Tdrb2+XAgQNisVh0+3n88celpKREOjo6pLm5WX77299KQEDAHX+mp06dks7OTvnf//6nO8ZQj507d8qlS5eks7NTHA6HHD58WAuhkdJjfwdvA0JEyo3Ka0RENLwwiIhIOQYRESnHICIi5RhERKQcg4iIlGMQEZFyDCIiUo5BRETKMYiISDkGEREp939znrzKwxRwJgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "lengths = []\n",
    "for row in ds['train']:\n",
    "  ids = tokenizer(row['text']).input_ids\n",
    "  lengths.append(len(ids))\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.hist(lengths, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIMuF5_D1_eC"
   },
   "source": [
    "Token 개수의 평균은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1724034382053,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "hgiiw0XaJvxk",
    "outputId": "a5868a8d-b0d1-45a0-fbfd-8e6b8e842ef0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "323.03248"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(lengths) / len(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j5FkKBL2jA_"
   },
   "source": [
    "## Dataloader 만들기\n",
    "\n",
    "이번에는 주어진 dataset `ds['train'], ds['test']`를 가지고 dataloader를 구현할 것입니다.\n",
    "Text data와 같은 data는 각각의 입력이 가변적이기 때문에 dataloader를 구현하는 것이 까다로워집니다.\n",
    "이처럼 matrix로 예쁘게 모을 수 없는 경우를 처리하기 위해 dataloader는 `collate_fn`이라는 인자로 batch를 처리할 수 있는 함수를 직접 구현할 수 있는 옵션을 제공합니다. 여기서 batch는 입력들의 list를 의미합니다.\n",
    "\n",
    "입력들의 list가 주어졌을 때 text들은 tokenize와 padding을 동시에 진행하여 하나의 matrix로 만드는 것은 다음과 같이 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "B3YIWRrGNgjI"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "  max_len = 400\n",
    "  texts, labels = [], []\n",
    "  for row in batch:\n",
    "    labels.append(row['label'])\n",
    "    texts.append(row['text'])\n",
    "\n",
    "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
    "  labels = torch.LongTensor(labels)\n",
    "\n",
    "  return texts, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnl9tN3p7OVQ"
   },
   "source": [
    "`tokenizer`의 인자로 들어가는 `truncation`과 `max_length`는 길이가 max_length를 넘는 data들은 max_length에서 자르라는 옵션입니다. 나머지는 위에서 설명한 tokenizer를 활용하는 방법과 동일합니다. Dataloader에서 원하는 collate_fn의 형태가 무엇인지 아는 것이 중요합니다.\n",
    "이렇게 구현한 `collate_fn`을 가지고 dataloader를 구현한 것은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "S8Iz3nkx6lI-"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vb5zJAAB7e15"
   },
   "source": [
    "다른 부분들은 이전 실습과 동일합니다. 차이는 `collate_fn`을 인자로 넘겨주는 부분입니다.\n",
    "\n",
    "이렇게 만들어진 dataloader에서 첫 번째 batch를 출력한 결과는 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1724034382053,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "4pb8ABt6OQ7t",
    "outputId": "a08af718-c4f6-4f4e-cd1f-660b98387c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 400]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "text, label = next(iter(train_loader))\n",
    "print(text.shape, label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGBvJpO17oKe"
   },
   "source": [
    "보시다시피 우리가 지정한 batch size로 잘 반환하는 것을 알 수 있습니다.\n",
    "여기서 text의 shape은 (B, S)의 형태인데, 이 S의 값은 batch 안에서 가장 긴 text로 결정되기 때문에 각각의 batch마다 값이 달라질 수 있다는 점 유의하시길 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4GToPIM_2l8p"
   },
   "source": [
    " ## RNN 구현\n",
    "\n",
    " 이번에는 본격적으로 주어진 token list가 긍정적인지 부정적인지를 출력해주는 model을 구현합니다.\n",
    " 이 model의 중요한 점은 세 가지입니다:\n",
    "\n",
    " 1. Token list를 `nn.Embedding`으로 전처리해줍니다.\n",
    " 2. `nn.RNN`으로 선언된 RNN을 통과시켜 representation을 얻습니다.\n",
    " 3. 주어진 token list의 마지막 token에 해당하는 representation을 얻어 `nn.Linear`를 통과시켜 출력을 구합니다.\n",
    "\n",
    "\n",
    "이를 구현한 것은 다음과 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Upf_MXf1OUIg"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "  def __init__(self, vocab_size, hidden_dim, n_layers):\n",
    "    super().__init__()\n",
    "\n",
    "    self.vocab_size = vocab_size\n",
    "    self.hidden_dim = hidden_dim\n",
    "    self.n_layers = n_layers\n",
    "\n",
    "    self.embedding = nn.Embedding(vocab_size, hidden_dim, padding_idx=tokenizer.pad_token_id)\n",
    "    self.rnn = nn.RNN(hidden_dim, hidden_dim, num_layers=n_layers, nonlinearity='relu', batch_first=True)\n",
    "    self.classifier = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "  def forward(self, x_org):\n",
    "    x = self.embedding(x_org)\n",
    "    x, _ = self.rnn(x)\n",
    "\n",
    "    lengths = (x_org != tokenizer.pad_token_id).sum(dim=-1) - 1 # 유효한것의 마지막 인덱스 가지고와서 형상 : [batch_size]\n",
    "    lengths = lengths[:, None, None].repeat(1, 1, self.hidden_dim) # 형태를 맞춰주기 형상 : [batch_size, 1,1]\n",
    "\n",
    "    x = x.gather(1, lengths) # 마지막 유효 은닉 상태 가져오기 형상 : [batch_size, 1,hidden_dim]\n",
    "    x = x[:, 0] # 차원 하나 없애기  형상 : [batch_size, hidden_dim]\n",
    "\n",
    "    x = self.classifier(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "model = TextClassifier(len(tokenizer), 32, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yR2P53tl8dDH"
   },
   "source": [
    "위의 두 줄은 dataloader로 부터 들고온 token list의 batch를 embedding하고 RNN을 태운 결과입니다.\n",
    "RNN은 입력이 (B, S, *)이면 output도 (B, S, *)이 됩니다.\n",
    "각각의 token에 대한 출력 결과를 기본적으로 반환합니다.\n",
    "여기서 중요한 것은 짧은 token list들은 뒷 부분이 padding token으로 채워져 있기 때문에 padding token이 등장하기 전 마지막 token의 출력 결과를 활용하여 text 분류를 진행해야 한다는 점입니다.\n",
    "즉, token list의 진짜 마지막 token에 해당하는 RNN output을 사용해야 합니다.\n",
    "이를 구현한 것이 `forward` 함수의 중간 부분입니다.\n",
    "먼저 batch의 각 입력의 마지막 token의 위치를 길이로 계산하여 얻습니다. 그리고 이를 `.gather`를 활용하여 들고옵니다.\n",
    "마지막으로 `x[:, 0]`를 호출하여 squeeze를 진행합니다.\n",
    "\n",
    "다음은 위에서 확인한 dataloader 첫 batch를 입력으로 넣었을 때, 출력의 shape입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1724034382053,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "_I3016jsJoeq",
    "outputId": "1299015b-5363-4d8d-fb28-75bd2ceabe59"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(text).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtAZhl6z2oUl"
   },
   "source": [
    "## 학습\n",
    "\n",
    "마지막으로 구현한 RNN을 가지고 학습하는 코드를 구현합니다. 학습 코드는 이전 MNIST 실습과 완전히 동일하기 때문에 설명없이 결과만 확인해보도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 676,
     "status": "ok",
     "timestamp": 1724034382723,
     "user": {
      "displayName": "조승혁",
      "userId": "15759752471844115325"
     },
     "user_tz": -540
    },
    "id": "YHVVsWBPQmnv",
    "outputId": "390560b9-4b74-4dc6-accd-ed73ea68fa70"
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "lr = 0.001\n",
    "# loss_fn = nn.CrossEntropyLoss()\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "HSSDAv8lRV5z"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def accuracy(model, dataloader):\n",
    "  cnt = 0\n",
    "  acc = 0\n",
    "\n",
    "  for data in dataloader:\n",
    "    inputs, labels = data\n",
    "\n",
    "    preds = model(inputs)\n",
    "    # preds = torch.argmax(preds, dim=-1)\n",
    "    preds = (preds > 0).long()[..., 0]\n",
    "\n",
    "    cnt += labels.shape[0]\n",
    "    acc += (labels == preds).sum().item()\n",
    "\n",
    "  return acc / cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "al_b56TYRILq",
    "outputId": "2c7ee484-6425-486f-e11e-e80498488451"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 | Train Loss: 267.4155042171478\n",
      "=========> Train acc: 0.609 | Test acc: 0.587\n",
      "Epoch   1 | Train Loss: 242.872698366642\n",
      "=========> Train acc: 0.721 | Test acc: 0.682\n",
      "Epoch   2 | Train Loss: 231.15738782286644\n",
      "=========> Train acc: 0.641 | Test acc: 0.609\n",
      "Epoch   3 | Train Loss: 219.43883627653122\n",
      "=========> Train acc: 0.724 | Test acc: 0.677\n",
      "Epoch   4 | Train Loss: 197.95575642585754\n",
      "=========> Train acc: 0.693 | Test acc: 0.664\n",
      "Epoch   5 | Train Loss: 2899.0923001766205\n",
      "=========> Train acc: 0.615 | Test acc: 0.584\n",
      "Epoch   6 | Train Loss: 239.9606828391552\n",
      "=========> Train acc: 0.747 | Test acc: 0.696\n",
      "Epoch   7 | Train Loss: 194.03296732902527\n",
      "=========> Train acc: 0.818 | Test acc: 0.775\n",
      "Epoch   8 | Train Loss: 171.98522692918777\n",
      "=========> Train acc: 0.845 | Test acc: 0.785\n",
      "Epoch   9 | Train Loss: 154.71272358298302\n",
      "=========> Train acc: 0.817 | Test acc: 0.742\n",
      "Epoch  10 | Train Loss: 145.06084513664246\n",
      "=========> Train acc: 0.873 | Test acc: 0.796\n",
      "Epoch  11 | Train Loss: 130.00611780583858\n",
      "=========> Train acc: 0.886 | Test acc: 0.803\n",
      "Epoch  12 | Train Loss: 126.48235308378935\n",
      "=========> Train acc: 0.873 | Test acc: 0.799\n",
      "Epoch  13 | Train Loss: 122.56977964937687\n",
      "=========> Train acc: 0.897 | Test acc: 0.805\n",
      "Epoch  14 | Train Loss: 103.97417455166578\n",
      "=========> Train acc: 0.913 | Test acc: 0.820\n",
      "Epoch  15 | Train Loss: 96.66396733373404\n",
      "=========> Train acc: 0.919 | Test acc: 0.831\n",
      "Epoch  16 | Train Loss: 93.43369937688112\n",
      "=========> Train acc: 0.929 | Test acc: 0.829\n",
      "Epoch  17 | Train Loss: 84.96188827604055\n",
      "=========> Train acc: 0.923 | Test acc: 0.813\n",
      "Epoch  18 | Train Loss: 81.79203891381621\n",
      "=========> Train acc: 0.933 | Test acc: 0.824\n",
      "Epoch  19 | Train Loss: 142.53421448543668\n",
      "=========> Train acc: 0.846 | Test acc: 0.741\n",
      "Epoch  20 | Train Loss: 98.53044071048498\n",
      "=========> Train acc: 0.899 | Test acc: 0.810\n",
      "Epoch  21 | Train Loss: 79.91214755177498\n",
      "=========> Train acc: 0.917 | Test acc: 0.817\n",
      "Epoch  22 | Train Loss: 76.12972988933325\n",
      "=========> Train acc: 0.948 | Test acc: 0.827\n",
      "Epoch  23 | Train Loss: 67.5020367577672\n",
      "=========> Train acc: 0.951 | Test acc: 0.832\n",
      "Epoch  24 | Train Loss: 58.91427709534764\n",
      "=========> Train acc: 0.961 | Test acc: 0.832\n",
      "Epoch  25 | Train Loss: 70.87465748935938\n",
      "=========> Train acc: 0.957 | Test acc: 0.830\n",
      "Epoch  26 | Train Loss: 70.74196860939264\n",
      "=========> Train acc: 0.963 | Test acc: 0.833\n",
      "Epoch  27 | Train Loss: 48.657815147191286\n",
      "=========> Train acc: 0.964 | Test acc: 0.824\n",
      "Epoch  28 | Train Loss: 47.01009673438966\n",
      "=========> Train acc: 0.952 | Test acc: 0.809\n",
      "Epoch  29 | Train Loss: 55.65421730931848\n",
      "=========> Train acc: 0.944 | Test acc: 0.818\n",
      "Epoch  30 | Train Loss: 67.01472866907716\n",
      "=========> Train acc: 0.975 | Test acc: 0.829\n",
      "Epoch  31 | Train Loss: 48.72845792211592\n",
      "=========> Train acc: 0.976 | Test acc: 0.823\n",
      "Epoch  32 | Train Loss: 36.2992598535493\n",
      "=========> Train acc: 0.979 | Test acc: 0.828\n",
      "Epoch  33 | Train Loss: 50.19998323917389\n",
      "=========> Train acc: 0.978 | Test acc: 0.816\n",
      "Epoch  34 | Train Loss: 56.69363683089614\n",
      "=========> Train acc: 0.971 | Test acc: 0.809\n",
      "Epoch  35 | Train Loss: 38.28568503912538\n",
      "=========> Train acc: 0.980 | Test acc: 0.816\n",
      "Epoch  36 | Train Loss: 27.369656899943948\n",
      "=========> Train acc: 0.984 | Test acc: 0.818\n",
      "Epoch  37 | Train Loss: 58.76890834886581\n",
      "=========> Train acc: 0.953 | Test acc: 0.790\n",
      "Epoch  38 | Train Loss: 31.48772050347179\n",
      "=========> Train acc: 0.952 | Test acc: 0.792\n",
      "Epoch  39 | Train Loss: 34.88332665618509\n",
      "=========> Train acc: 0.987 | Test acc: 0.828\n",
      "Epoch  40 | Train Loss: 29.96948791667819\n",
      "=========> Train acc: 0.970 | Test acc: 0.824\n",
      "Epoch  41 | Train Loss: 31.585858623031527\n",
      "=========> Train acc: 0.976 | Test acc: 0.798\n",
      "Epoch  42 | Train Loss: 40.908915924374014\n",
      "=========> Train acc: 0.989 | Test acc: 0.823\n",
      "Epoch  43 | Train Loss: 33.934715535491705\n",
      "=========> Train acc: 0.989 | Test acc: 0.822\n",
      "Epoch  44 | Train Loss: 53.17271953262389\n",
      "=========> Train acc: 0.989 | Test acc: 0.813\n",
      "Epoch  45 | Train Loss: 26.64894051477313\n",
      "=========> Train acc: 0.954 | Test acc: 0.787\n",
      "Epoch  46 | Train Loss: 16.732667794683948\n",
      "=========> Train acc: 0.994 | Test acc: 0.821\n",
      "Epoch  47 | Train Loss: 17.86713416292332\n",
      "=========> Train acc: 0.991 | Test acc: 0.816\n",
      "Epoch  48 | Train Loss: 35.22381112072617\n",
      "=========> Train acc: 0.983 | Test acc: 0.819\n",
      "Epoch  49 | Train Loss: 65.27634543040767\n",
      "=========> Train acc: 0.991 | Test acc: 0.817\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "  total_loss = 0.\n",
    "  model.train()\n",
    "  for data in train_loader:\n",
    "    model.zero_grad()\n",
    "    inputs, labels = data\n",
    "    inputs, labels = inputs.to('cuda'), labels.to('cuda').float()\n",
    "\n",
    "    preds = model(inputs)[..., 0]\n",
    "    loss = loss_fn(preds, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    total_loss += loss.item()\n",
    "\n",
    "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")\n",
    "\n",
    "  with torch.no_grad():\n",
    "    model.eval()\n",
    "    train_acc = accuracy(model, train_loader)\n",
    "    test_acc = accuracy(model, test_loader)\n",
    "    print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nK-0-TMS9xde"
   },
   "source": [
    "불안정한 부분들이 있지만, 잘 수렴하는 것을 확인할 수 있습니다. 이전 챕터에서 배운 LSTM이나 GRU는 PyTorch에서 `nn.LSTM`과 `nn.GRU`로 잘 구현되어있습니다. `nn.RNN`의 인자들을 그대로 활용할 수 있기 때문에 코드상에서 대체하여 바로 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K9H-KxcGQ8iK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "langchain-kr-_ls1lJm0-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
