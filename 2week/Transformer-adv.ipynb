{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymxatB5WYxlL"
   },
   "source": [
    "데이터 정의 및 기존 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HOdhoBVA1zcu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/yunhyeokchoi/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 398]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "ds = load_dataset(\"stanfordnlp/imdb\")\n",
    "tokenizer = torch.hub.load(\n",
    "    \"huggingface/pytorch-transformers\", \"tokenizer\", \"bert-base-uncased\"\n",
    ")\n",
    "\n",
    "\n",
    "# 마지막 단어 제외하고, 해당 단어를 라벨에 넣는 collate 함수\n",
    "def collate_fn_2(batch):\n",
    "    max_len = 400\n",
    "    texts, labels = [], []\n",
    "    for row in batch:\n",
    "        input_ids = tokenizer(\n",
    "            row[\"text\"], truncation=True, max_length=max_len\n",
    "        ).input_ids\n",
    "\n",
    "        # 구두점은 텍스트에 남겨두고 라벨 후보에서만 제외\n",
    "        if len(input_ids) > 2:\n",
    "            candidate_label = input_ids[-2]\n",
    "            if candidate_label not in {1012, 999, 1028, 102, 101}:\n",
    "                labels.append(candidate_label)\n",
    "            else:\n",
    "                labels.append(\n",
    "                    input_ids[-3]\n",
    "                )  # 마지막 두 번째 단어가 의미 없으면 그 앞 단어 선택\n",
    "        else:\n",
    "            labels.append(tokenizer.pad_token_id)\n",
    "\n",
    "        texts.append(torch.LongTensor(input_ids[:-2]))  # 마지막 두 단어 제외\n",
    "\n",
    "    texts = pad_sequence(texts, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    labels = torch.LongTensor(labels)\n",
    "\n",
    "    return texts, labels\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds[\"train\"], batch_size=64, shuffle=True, collate_fn=collate_fn_2\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    ds[\"test\"], batch_size=64, shuffle=False, collate_fn=collate_fn_2\n",
    ")\n",
    "\n",
    "text, label = next(iter(train_loader))\n",
    "print(text.shape, label.shape)\n",
    "\n",
    "max_word_len = 398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MBlMVMZcRAxv"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 결과 출력을 위한 모니터\n",
    "class AccuracyMonitor:\n",
    "    def __init__(\n",
    "        self, models, dataloaders, labels, title=\"Model Accuracies\", accuracy_fn=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        models: 모델 리스트\n",
    "        dataloaders: 데이터로더 리스트\n",
    "        labels: 모델 및 데이터로더에 대한 레이블 리스트\n",
    "        title: 그래프 제목\n",
    "        accuracy_fn: 사용자 정의 정확도 함수 (default_accuracy를 기본값으로 사용)\n",
    "        \"\"\"\n",
    "        if not (len(models) == len(dataloaders) == len(labels)):\n",
    "            raise ValueError(\n",
    "                \"models, dataloaders, labels는 모두 같은 길이를 가져야 합니다.\"\n",
    "            )\n",
    "\n",
    "        self.models = models\n",
    "        self.dataloaders = dataloaders\n",
    "        self.labels = labels\n",
    "        self.title = title\n",
    "        self.acc_lists = [[] for _ in labels]\n",
    "        self.accuracy_fn = accuracy_fn if accuracy_fn else self.default_accuracy\n",
    "\n",
    "    def default_accuracy(self, model, dataloader, **kwargs):\n",
    "        \"\"\"\n",
    "        기본 정확도 계산 함수. **kwargs를 통해 추가 옵션 지원.\n",
    "        \"\"\"\n",
    "        cnt = 0\n",
    "        acc = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data in dataloader:\n",
    "                inputs, labels = data\n",
    "                preds = model(inputs)\n",
    "                preds = torch.argmax(preds, dim=-1)\n",
    "                cnt += labels.shape[0]\n",
    "                acc += (labels == preds).sum().item()\n",
    "        model.train()\n",
    "        return acc / cnt if cnt > 0 else 0.0\n",
    "\n",
    "    def update_accuracies(self, verbose=False, **kwargs):\n",
    "        \"\"\"\n",
    "        models와 dataloaders를 평가하고 acc_lists에 기록.\n",
    "        verbose: True인 경우 정확도 업데이트 로그 출력.\n",
    "        \"\"\"\n",
    "        for i, (model, dataloader) in enumerate(zip(self.models, self.dataloaders)):\n",
    "            acc = self.accuracy_fn(model, dataloader, **kwargs)\n",
    "            self.acc_lists[i].append(acc)\n",
    "            if verbose:\n",
    "                print(f\"Updated Accuracy ({self.labels[i]}): {acc:.4f}\")\n",
    "\n",
    "    def plot(self, epoch=None, save_path=None):\n",
    "        \"\"\"\n",
    "        정확도 그래프를 출력하고 저장하는 기능.\n",
    "        epoch: 그래프 제목에 표시할 에폭 정보\n",
    "        save_path: 그래프를 저장할 경로 (None이면 저장하지 않음)\n",
    "        \"\"\"\n",
    "        if not self.acc_lists or len(self.acc_lists[0]) == 0:\n",
    "            print(\"No accuracies to plot\")\n",
    "            return\n",
    "\n",
    "        x = np.arange(len(self.acc_lists[0]))  # 에폭 수만큼 x축 생성\n",
    "\n",
    "        plt.figure(figsize=(10, 6))  # 그래프 크기 조정\n",
    "        for acc_list, label in zip(self.acc_lists, self.labels):\n",
    "            plt.plot(x, acc_list, label=label, marker=\"o\")\n",
    "\n",
    "        title = f\"{self.title} at Epoch {epoch}\" if epoch is not None else self.title\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.ylim(0.0, 1.0)\n",
    "\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Plot saved to {save_path}\")\n",
    "        else:\n",
    "            plt.show()\n",
    "\n",
    "# MPS 사용하기 위한 설정\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_built() else torch.device(\"cpu\")\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.wq = nn.Linear(\n",
    "            input_dim, d_model\n",
    "        )  \n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        q, k, v = self.wq(x), self.wk(x), self.wv(x)\n",
    "        score = torch.matmul(\n",
    "            q, k.transpose(-1, -2)\n",
    "        )  # (B, S, D) * (B, D, S) = (B, S, S)\n",
    "        score = score / sqrt(self.d_model)\n",
    "\n",
    "        if mask is not None:\n",
    "            score = score + (mask * -1e9)\n",
    "\n",
    "        score = self.softmax(score)\n",
    "        result = torch.matmul(score, v)\n",
    "        result = self.dense(result)\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "class SATransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, dff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.dff = dff\n",
    "\n",
    "        self.sa = SelfAttention(input_dim, d_model)\n",
    "        self.ffn = nn.Sequential(  # 토큰 백터를 개별적으로 처리후 빈선형 변환을 통해 풍부한 표현력 제공\n",
    "            nn.Linear(d_model, dff), nn.ReLU(), nn.Linear(dff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sa(x, mask)\n",
    "        x = self.ffn(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "def get_angles(pos, i, d_model):\n",
    "    angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return pos * angle_rates\n",
    "\n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, None], np.arange(d_model)[None, :], d_model\n",
    "    )\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    pos_encoding = angle_rads[None, ...]\n",
    "\n",
    "    return torch.FloatTensor(pos_encoding)\n",
    "\n",
    "class SALastWordPrediction(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, dff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.dff = dff\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.parameter.Parameter(\n",
    "            positional_encoding(max_word_len, d_model), requires_grad=False\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [SATransformerLayer(d_model, d_model, dff) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.classification = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x == tokenizer.pad_token_id).unsqueeze(1)\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = x * sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = self.classification(x)\n",
    "        x = x[:, -1, :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3VYrqTJagS1"
   },
   "source": [
    "## [MY CODE] Multi Head Attention 구현해 마지막 단어 예측하는 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_head):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.d_devied = d_model // n_head\n",
    "\n",
    "        # Linear layers for Q, K, V\n",
    "        self.wq = nn.Linear(input_dim, d_model)  \n",
    "        self.wk = nn.Linear(input_dim, d_model)\n",
    "        self.wv = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Final linear layer\n",
    "        self.dense = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Softmax\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "        seq_len = x.size(1)\n",
    "\n",
    "        # Linear transformations\n",
    "        q = self.wq(x).view(batch_size, seq_len, self.n_head, self.d_devied).transpose(1, 2)  # (B, H, S, D')\n",
    "        k = self.wk(x).view(batch_size, seq_len, self.n_head, self.d_devied).transpose(1, 2)\n",
    "        v = self.wv(x).view(batch_size, seq_len, self.n_head, self.d_devied).transpose(1, 2)\n",
    "\n",
    "        # Attention scores\n",
    "        score = torch.matmul(q, k.transpose(-1,-2)) / sqrt(self.d_devied)  # (B, H, S, S)\n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand(-1, self.n_head, -1, -1)  # mask 도 차원 변환(B, H, 1, S)\n",
    "            score = score + (mask * -1e9)\n",
    "\n",
    "        # Attention weights\n",
    "        score = self.softmax(score)\n",
    "\n",
    "        # Weighted sum\n",
    "        result = torch.matmul(score, v)  # (B, H, S, D')\n",
    "\n",
    "        # Concatenate heads and project\n",
    "        result = result.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)  # (B, S, D)\n",
    "        output = self.dense(result)\n",
    "\n",
    "        return output\n",
    "    \n",
    "\n",
    "class MHATransformerLayer(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, n_head, dff, dropout_prob=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.d_model = d_model\n",
    "        self.n_head = n_head\n",
    "        self.dff = dff\n",
    "\n",
    "        self.MHA = Multi_Head_Attention(input_dim, d_model, n_head)\n",
    "        self.ffn = nn.Sequential(  # 토큰 백터를 개별적으로 처리후 비선형 변환을 통해 풍부한 표현력 제공\n",
    "            nn.Linear(d_model, dff), nn.ReLU(), nn.Linear(dff, d_model))\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.normal1 = nn.LayerNorm(d_model)\n",
    "        self.normal2 = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "\n",
    "        x1 = self.MHA(x, mask)\n",
    "        x1 = self.dropout(x1)\n",
    "        x1 = self.normal1(x1 + x)\n",
    "\n",
    "        x2 = self.ffn(x1)\n",
    "        x2 = self.dropout(x2)\n",
    "        x2 = self.normal2(x2 + x1)\n",
    "\n",
    "        return x2\n",
    "    \n",
    "\n",
    "class MHALastWordPrediction(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_head, dff):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        self.n_layers = n_layers\n",
    "        self.dff = dff\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = nn.parameter.Parameter(\n",
    "            positional_encoding(max_word_len, d_model), requires_grad=False\n",
    "        )\n",
    "        self.layers = nn.ModuleList(\n",
    "            [MHATransformerLayer(d_model, d_model,n_head, dff) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.classification = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mask = (x == tokenizer.pad_token_id).unsqueeze(1)\n",
    "        seq_len = x.shape[1]\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x = x * sqrt(self.d_model)\n",
    "        x = x + self.pos_encoding[:, :seq_len]\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "\n",
    "        x = self.classification(x)\n",
    "        x = x[:, -1, :]\n",
    "        return x\n",
    "\n",
    "\n",
    "SA_model = SALastWordPrediction(len(tokenizer), 16, 2, 32)\n",
    "MHA_model = MHALastWordPrediction(len(tokenizer), 16, 2, 4, 32)\n",
    "lr = 0.01\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "SA_optimizer = Adam(SA_model.parameters(), lr=lr)\n",
    "MHA_optimizer = Adam(MHA_model.parameters(), lr=lr)\n",
    "\n",
    "SA_model = SA_model.to(device)\n",
    "MHA_model = MHA_model.to(device)\n",
    "\n",
    "def custom_accuracy(model, dataloader):\n",
    "    cnt = 0\n",
    "    acc = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        preds = model(inputs)\n",
    "        # preds = torch.argmax(preds, dim=-1)\n",
    "        preds = torch.argmax(preds, dim=-1)  # 가장 높은 확률을 가진 클래스를 선택\n",
    "\n",
    "        cnt += labels.shape[0]\n",
    "        acc += (labels == preds).sum().item()\n",
    "\n",
    "    return acc / cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [MY CODE] 마지막 단어 예측 Self Attention, Multi Head Attention 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "8MaiCGh8TsDH"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71fbf66b23f34e458d42902ecec307cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/50:   0%|          | 0/391 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m     MHAloss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     31\u001b[0m     MHA_optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_postfix(SA_loss\u001b[38;5;241m=\u001b[39m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, MHA_loss\u001b[38;5;241m=\u001b[39mMHAloss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     37\u001b[0m     SA_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "\n",
    "trans_monitor = AccuracyMonitor(\n",
    "    models=[SA_model, MHA_model],\n",
    "    dataloaders=[train_loader, train_loader],\n",
    "    labels=[\"Self\", \"Multi\"],\n",
    "    accuracy_fn=custom_accuracy,\n",
    ")\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{n_epochs}\", unit=\"batch\") as pbar:\n",
    "        SA_model.train()\n",
    "        MHA_model.train()\n",
    "        for data in train_loader:\n",
    "\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            labels = labels.long()\n",
    "\n",
    "            SA_model.zero_grad()\n",
    "            preds = SA_model(inputs)\n",
    "            loss = loss_fn(preds, labels)\n",
    "            loss.backward()\n",
    "            SA_optimizer.step()\n",
    "\n",
    "            MHA_model.zero_grad()\n",
    "            MHApreds = MHA_model(inputs)\n",
    "            MHAloss = loss_fn(MHApreds, labels)\n",
    "            MHAloss.backward()\n",
    "            MHA_optimizer.step()\n",
    "\n",
    "            pbar.set_postfix(SA_loss=loss.item(), MHA_loss=MHAloss.item())\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            SA_model.eval()\n",
    "            MHA_model.eval()\n",
    "            trans_monitor.update_accuracies()\n",
    "\n",
    "trans_monitor.plot()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "langchain-kr-_ls1lJm0-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
